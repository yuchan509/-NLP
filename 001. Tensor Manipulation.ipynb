{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 기본 구성\n",
    "- **torch** : 메인 네임스페이스로 텐서 등의 다양한 수학 함수가 포함되어져 있으며 Numpy와 유사한 구조 가짐.\n",
    "- **torch.autograd** : 자동 미분을 위한 함수들이 포함됨. 자동 미분의 on/off를 제어하는 콘텍스트 매니저(enable_grad/no_grad)나 자체 미분 가능 함수를 정의할 때 사용하는 기반 클래스인 'Function' 등이 포함.\n",
    "- **torch.nn** : 신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있음. RNN, LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들이 존재함.\n",
    "- **torch.optim** : 확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘이 구현.\n",
    "- **torch.utils.data** : SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수가 포함.\n",
    "- **torch.onnx** : ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용합니다. ONNX는 서로 다른 딥 러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 조작하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scala : 차원이 존재하지 않는 값.\n",
    "- Vector : 1차원으로 구성된 값. 1차원 Tensor로도 표현이 가능.\n",
    "- Matrix : 2차원으로 구성된 값. 2차원 Tensor로도 표현이 가능.\n",
    "- Tensor : 3차원 이상으로 구성된 값."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Tensor Shape Convention\n",
    "- 딥 러닝을 할때 다루고 있는 행렬 또는 텐서의 크기를 고려하는 것은 항상 중요.\n",
    "    - 2D Tensor(Typical Simple Setting) : 2차원 텐서의 크기 |t|를 (batch size × dimension)으로 표현.\n",
    "    - 3D Tensor(Typical Computer Vision) : 비전 분야에서의 3차원 Tensor.\n",
    "    - **3D Tensor(Typical Natural Language Processing) - NLP 분야에서의 3차원 Tensor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]], [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]\n",
      "[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]], [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]\n"
     ]
    }
   ],
   "source": [
    "text = [['나는 사과를 좋아해'], ['나는 바나나를 좋아해'], ['나는 사과를 싫어해'], ['나는 바나나를 싫어해']]\n",
    "\n",
    "\n",
    "# 각 리스트 내의 문장을 단어별로 분리.\n",
    "# 결과 :  4 × 3의 크기를 가지는 2D Tensor : 4 x 3 matrix.\n",
    "text = [''.join(text[i]).split() for i, j in enumerate(text)]\n",
    "\n",
    "\n",
    "# 단어별 텍스트 문자 벡터화.\n",
    "# '나는'    = [0.1, 0.2, 0.9]\n",
    "# '바나나를' = [0.3, 0.5, 0.2]\n",
    "# '사과를'  = [0.3, 0.5, 0.1]\n",
    "# '좋아해'  = [0.7, 0.6, 0.5]\n",
    "# '싫어해'  = [0.5, 0.6, 0.7]\n",
    "\n",
    "# Train data 재구성. : 4 x 3 x 3 3D Tesor\n",
    "Text_Vector = [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],\n",
    "             [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],\n",
    "             [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],\n",
    "             [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]\n",
    "\n",
    "# 2개의 배치 사이즈로 구성.\n",
    "# 각 배치의 텐서의 크기는 2 × 3 × 3. 이는 (batch size, 문장 길이, 단어 벡터의 차원)의 크기를 의미함.\n",
    "Batch1 = Text_Vector[: 2]\n",
    "Batch2 = Text_Vector[2:]\n",
    "print(Batch1)\n",
    "print(Batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 텐서 선언하기.\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D with PyToch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
      "rank  : 1\n",
      "shape : torch.Size([7])\n",
      "size  : torch.Size([7])\n",
      "tensor([2., 3.]) tensor([4.])\n",
      "tensor(0.) tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# 1차원 벡터 만들기.\n",
    "t = torch.FloatTensor(list(range(7)))\n",
    "print(t)\n",
    "\n",
    "# rank, dim\n",
    "print(f'rank  : {t.dim()}')\n",
    "\n",
    "# shape.\n",
    "print(f'shape : {t.shape}')\n",
    "\n",
    "# size = shape\n",
    "print(f'size  : {t.size()}')\n",
    "\n",
    "# slicing.\n",
    "print(t[2 : 4], t[4 : -2])\n",
    "\n",
    "# indexing.\n",
    "print(t[0], t[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D with PyToch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "rank  : 2\n",
      "shape : torch.Size([4, 3])\n",
      "size  : torch.Size([4, 3])\n",
      "tensor([ 2.,  5.,  8., 11.])\n",
      "tensor([1., 2., 3.]) tensor([7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "# 2차원 벡터 만들기.\n",
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "print(t)\n",
    "\n",
    "# rank, dim\n",
    "print(f'rank  : {t.dim()}')\n",
    "\n",
    "# shape.\n",
    "print(f'shape : {t.shape}')\n",
    "\n",
    "# size = shape\n",
    "print(f'size  : {t.size()}')\n",
    "\n",
    "# slicing.\n",
    "# 2행 요소값만을 모두 가져옴.\n",
    "print(t[:, 1])\n",
    "\n",
    "# indexing.\n",
    "print(t[0], t[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "- 행렬의 사칙 연산을 쉽게 수행할 수 있도록 PyTorch 내에서 자동으로 크기를 맞춰 연산을 수행하게 도와줌.\n",
    "- 자동으로 실행되는 기능이므로 결과 문제 발생시, 어디서 문제가 발생했는지 찾기 어려울 수 있므로 주의하여 이용할 필요가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# 두 행렬의 크기가 같으나, 수학적으로 원래 연산이 불가.\n",
    "m1 = torch.FloatTensor([3, 3])\n",
    "m2 = torch.FloatTensor([2, 2])\n",
    "print( m1 + m2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7., 8.])\n"
     ]
    }
   ],
   "source": [
    "# vector + scalar\n",
    "m3 = torch.FloatTensor([2, 3])\n",
    "\n",
    "# broadcasting에 의해 [5] => [5, 5] : 차원의 크기를 맞춰줌.\n",
    "m4 = torch.FloatTensor([5])\n",
    "print( m3 + m4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 2 x 1 vector + 1 x 2 vector\n",
    "# 수학적으로 계산이 불가한 연산.\n",
    "# broadcasting에 의해 두 벡터의 크기를 2 x 2로 수정하여 계산함.\n",
    "# [1, 2] => [[1, 2], [1, 2]]\n",
    "m5 = torch.FloatTensor([[1, 2]])\n",
    "\n",
    "# [3], [4] => [[3, 3], [4, 4]]\n",
    "m6 = torch.FloatTensor([[3], [4]])\n",
    "print( m5 + m6 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication Vs Multiplication\n",
    "- Matrix Multiplication : matmul()\n",
    "- Multiplication : mul() or *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1:  torch.Size([2])\n",
      "Shape of Matrix 2:  torch.Size([2])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplication : 일반적인 행렬의 곱셈을 계산함. \n",
    "\n",
    "m7 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m8 = torch.FloatTensor([[1], [2]])\n",
    "\n",
    "# 2 x 2\n",
    "print('Shape of Matrix 1: ', m1.shape)\n",
    "\n",
    "# 2 x 1\n",
    "print('Shape of Matrix 2: ', m2.shape) \n",
    "\n",
    "# 2 x 1\n",
    "print( m7.matmul(m8) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# Multiplication : element-wise 곱셈이 수행됨.\n",
    "print( m7.mul(m8) ) \n",
    "print( m7 * m8 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n",
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "# 1차원인 경우.\n",
    "t = torch.FloatTensor([1, 2])\n",
    "print(t.mean())\n",
    "\n",
    "# 2차원인 경우.\n",
    "t2 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t2.mean())\n",
    "\n",
    "# 차원을 인자로 주어 각 행 또는 열별로 평균을 계산.\n",
    "# 행을 기준 : 0, 열을 기준 : 1\n",
    "print(t2.mean(dim = 0))\n",
    "print(t2.mean(dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "# 단순히 원소 전체의 덧셈을 수행.\n",
    "print(t2.sum()) \n",
    "\n",
    "# 열별로 덧셈.\n",
    "print(t2.sum(dim = 0))\n",
    "\n",
    "# 행별로 뎃셈.\n",
    "print(t2.sum(dim = 1)) \n",
    "\n",
    "# 행별로 덧셈.\n",
    "print(t2.sum(dim = -1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max& ArgMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "\n",
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "\n",
      " ArgMax : tensor([1, 1])\n",
      "\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t2.max())\n",
    "print()\n",
    "\n",
    "# 각 열별로 가장 큰값과 해당 인덱스값(argmax)을 반환.\n",
    "print(t2.max(dim = 0))\n",
    "print()\n",
    "print(f' ArgMax : {t2.max(dim = 0)[1]}')\n",
    "print()\n",
    "\n",
    "# 각 행별로 가장 큰값과 해당 인덱스값을 반환.\n",
    "print(t2.max(dim = 1))\n",
    "print(t2.max(dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View\n",
    "- 원소의 수를 유지하면서 텐서의 크기 변경.\n",
    "- reshape의 역할."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n",
      "\n",
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 x 2 x 3\n",
    "t3 = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "\n",
    "ft = torch.FloatTensor(t3)\n",
    "print(ft.shape)\n",
    "\n",
    "# 모든 원소의 수 : 12개.\n",
    "# 3차원(2 x 2 x 3) -> 2차원(2 * 2, 3) = (4, 3)\n",
    "# -1로 설정되면 다른 차원으로부터 해당값을 유추.\n",
    "print(ft.view([-1, 3]))\n",
    "print(ft.view([-1, 3]).shape)\n",
    "print()\n",
    "\n",
    "# 3차원 크기 변경.\n",
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze\n",
    "- 스퀴즈는 차원이 1인 경우 해당 차원을 제거."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft2 = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft2.shape)\n",
    "\n",
    "# Squeeze.\n",
    "# 2차원 벡터가 1차원 벡터로 변경.\n",
    "print(ft2.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsqueeze\n",
    "- Squeeze의 반대 개념.\n",
    "- 특정 위치에 1인 차원 추가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# index가 0부터 시작하므로, 0은 첫번째 차원을 의미함.\n",
    "ft3 = torch.FloatTensor([0, 1, 2])\n",
    "print(ft3.shape)\n",
    "\n",
    "# Unsqueeze.\n",
    "# 첫번째 차원에 차원을 추가.\n",
    "print(ft3.unsqueeze(0).shape)\n",
    "\n",
    "# 두번째 차원에 차원을 추가.\n",
    "print(ft3.unsqueeze(1).shape)\n",
    "print(ft3.unsqueeze(-1).shape)\n",
    "\n",
    "# View.\n",
    "print(ft3.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Casting\n",
    "- Tensor에는 자료형이라는 것이 존재. ex) torch.FloatTensor, torch.LongTensor, torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "tensor([1., 0., 0., 1.])\n",
      "tensor([1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "\n",
    "# float 타입으로 변경.\n",
    "print(lt.float())\n",
    "\n",
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt.float())\n",
    "\n",
    "# long 타입으로 변경.\n",
    "print(bt.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate\n",
    "- 두 텐서를 연결."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "\n",
    "# dim = 0 : 행을 기준.(4 x 2)\n",
    "print(torch.cat([x, y], dim = 0))\n",
    "\n",
    "# dim = 1 : 열을 기준.(2 x 4)\n",
    "print(torch.cat([x, y], dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "- 두 텐서를 연결시키는 또 하나의 방법."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# (2,)\n",
    "x1 = torch.FloatTensor([1, 4])\n",
    "y1 = torch.FloatTensor([2, 5])\n",
    "z1 = torch.FloatTensor([3, 6])\n",
    "\n",
    "# (1, 2) 벡터로 변경후 cat 사용.\n",
    "print(torch.cat([x1.unsqueeze(0), y1.unsqueeze(0), z1.unsqueeze(0)], dim=0))\n",
    "\n",
    "# (3, 2)\n",
    "print(torch.stack([x1, y1, z1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ones_like & zeros_like\n",
    "- ones_like : 1로만 채워진 텐서를 생성.\n",
    "- zeros_like : 0으로만 채워진 텐서를 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n",
      "\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x2)\n",
    "print()\n",
    "\n",
    "# 입력 텐서와 크기를 동일하게 하면서 값을 1로 채우기.\n",
    "print(torch.ones_like(x2))\n",
    "print()\n",
    "\n",
    "# 입력 텐서와 크기를 동일하게 하면서 값을 0로 채우기.\n",
    "print(torch.zeros_like(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In - place Operation\n",
    "- 변수의 변경된 값을 적용.(덮어쓰기 연산) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# 2 x 2 tensor.\n",
    "x3 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "\n",
    "# 2를 곱한 결과 출력.\n",
    "print(x3.mul(2))\n",
    "\n",
    "# 기존 값 출력.\n",
    "print(x3)\n",
    "print()\n",
    "\n",
    "# 2를 곱한 결과 출력.\n",
    "# _ 추가함으로써 inplace True값 적용.\n",
    "print(x3.mul_(2))\n",
    "\n",
    "# 기존 값 출력.\n",
    "print(x3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
